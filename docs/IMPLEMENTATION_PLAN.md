# Complex Use Case Implementation Plan
# 6 Tests: Python Golden Model + RTL Verification

## Status Overview

| # | Use Case | Python Model | RTL Test | Status |
|---|----------|--------------|----------|--------|
| 1 | Residual Add | âœ… Done | âœ… Done | **COMPLETE** |
| 2 | Batch Processing | âœ… Done | ğŸ”„ Debug | In Progress |
| 3 | 2-Layer MLP | â¬š TODO | â¬š TODO | Pending |
| 4 | Large Tiled GEMM | â¬š TODO | â¬š TODO | Pending |
| 5 | Multi-Channel Conv2D | â¬š TODO | â¬š TODO | Pending |
| 6 | Attention Score | â¬š TODO | â¬š TODO | Pending |

---

## Implementation Order (by complexity)

### Phase 1: Foundation Tests (Simple VPU + GEMM patterns)
1. **Residual Add** âœ… - Skip connection: Y = ReLU(XÃ—W+b) + X
2. **Batch Processing** ğŸ”„ - Weight reuse: Y[n] = ReLU(X[n]Ã—W+b)

### Phase 2: Multi-Stage Compute
3. **2-Layer MLP** - Layer chaining with DDR handoff
4. **Large Tiled GEMM** - K-accumulation across tiles

### Phase 3: Advanced Patterns
5. **Multi-Channel Conv2D** - Channel accumulation via im2col
6. **Attention Score** - QÃ—K^T, normalization, Ã—V

---

## Test 1: Residual Add âœ… COMPLETE

**Files:**
- `python/models/model_residual.py` âœ…
- `tb/tb_residual_block.v` âœ…

**Verification:** PASSED

---

## Test 2: Batch Processing

**Concept:** Process N samples with shared weights
```
Y[0] = ReLU(X[0] Ã— W + b)
Y[1] = ReLU(X[1] Ã— W + b)
...
Y[N-1] = ReLU(X[N-1] Ã— W + b)
```

**Key Insight:** Single GEMM processes all samples at once!
- X_batch[NÃ—F] Ã— W[FÃ—F] = Z[NÃ—F]
- Each row of Z is one sample's output

**Files:**
- `python/models/model_batch_inference.py` âœ…
- `tb/tb_batch_inference.v` ğŸ”„

**Debug Note:** GEMM output correct, need to verify VPU bias add per-row

---

## Test 3: 2-Layer MLP

**Concept:** Two sequential linear layers with activation
```
H = ReLU(X Ã— W1 + b1)    # Layer 1: Input â†’ Hidden
Y = ReLU(H Ã— W2 + b2)    # Layer 2: Hidden â†’ Output
```

**Parameters:**
- X: 4Ã—4 (batch=4, in_features=4)
- W1: 4Ã—8, b1: 8 (hidden_dim=8)
- W2: 8Ã—4, b2: 4 (out_features=4)
- Y: 4Ã—4

**Data Flow:**
1. Load X, W1, b1 to SRAM
2. GEMM: Z1 = X Ã— W1
3. VPU: H = ReLU(Z1 + b1)
4. Store H to SRAM (or keep in place)
5. Load W2, b2
6. GEMM: Z2 = H Ã— W2
7. VPU: Y = ReLU(Z2 + b2)

**Files:**
- `python/models/model_mlp_2layer.py`
- `tb/tb_mlp_2layer.v`

---

## Test 4: Large Tiled GEMM (16Ã—16)

**Concept:** Matrix too large for single GEMM, tiled with K-accumulation
```
C[16Ã—16] = A[16Ã—16] Ã— B[16Ã—16]

Tile size: 4Ã—4
Tiles per dim: 4
K-tiles: 4 partial products per output tile
```

**Algorithm:**
```
for i in [0,1,2,3]:      # Output tile rows
  for j in [0,1,2,3]:    # Output tile cols
    C_tile[i,j] = 0
    for k in [0,1,2,3]:  # K-dimension tiles
      C_tile[i,j] += A_tile[i,k] Ã— B_tile[k,j]
```

**Key Operations:**
- GEMM for each partial product
- VPU ADD for K-accumulation (already verified!)

**Files:**
- `python/models/model_tiled_gemm.py`
- `tb/tb_tiled_gemm_16x16.v`

---

## Test 5: Multi-Channel Conv2D

**Concept:** Convolution with multiple input/output channels via im2col
```
Input:  [Ci, H, W] = [2, 6, 6]
Kernel: [Co, Ci, Kh, Kw] = [2, 2, 3, 3]
Output: [Co, Ho, Wo] = [2, 4, 4]
```

**im2col Transform:**
```
Patches: [CiÃ—KhÃ—Kw, HoÃ—Wo] = [18, 16]
Weights: [Co, CiÃ—KhÃ—Kw] = [2, 18]
Output:  Weights Ã— Patches = [2, 16] â†’ reshape [2, 4, 4]
```

**Simplified Test (fits in 4Ã—4 systolic):**
- Input: [2, 4, 4] - 2 channels, 4Ã—4 spatial
- Kernel: 2Ã—2 with 2 input, 2 output channels
- Output: [2, 3, 3]

**Files:**
- `python/models/model_conv2d_multi.py`
- `tb/tb_conv2d_multichannel.v`

---

## Test 6: Attention Score (Simplified)

**Concept:** Core attention mechanism
```
Attention(Q, K, V) = softmax(Q Ã— K^T / âˆšd) Ã— V
```

**Simplified (no true softmax):**
```
S = Q Ã— K^T           # Score matrix
S' = ReLU(S)          # Approximate "attention" (simplified)
O = S' Ã— V            # Output
```

**Parameters:**
- Sequence length: 4
- Head dimension: 4
- Q, K, V: 4Ã—4 each

**True softmax requires:**
- Row-wise max (VPU MAX reduction)
- Subtract max (VPU SUB)
- Exp approximation (piecewise linear or lookup)
- Sum reduction and divide

For POC, we'll use ReLU-attention as approximation.

**Files:**
- `python/models/model_attention.py`
- `tb/tb_attention.v`

---

## Golden Vector Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Python Model       â”‚
â”‚  (numpy reference)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  generate_golden()  â”‚
â”‚  - Input matrices   â”‚
â”‚  - Expected outputs â”‚
â”‚  - Intermediate valsâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  golden_vectors/    â”‚
â”‚  - test_X.hex       â”‚
â”‚  - test_W.hex       â”‚
â”‚  - test_Y_golden.hexâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RTL Testbench      â”‚
â”‚  - $readmemh()      â”‚
â”‚  - Compare outputs  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## File Structure

```
tensor_accelerator/
â”œâ”€â”€ python/
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ model_residual.py        âœ…
â”‚       â”œâ”€â”€ model_batch_inference.py âœ…
â”‚       â”œâ”€â”€ model_mlp_2layer.py      TODO
â”‚       â”œâ”€â”€ model_tiled_gemm.py      TODO
â”‚       â”œâ”€â”€ model_conv2d_multi.py    TODO
â”‚       â””â”€â”€ model_attention.py       TODO
â”œâ”€â”€ golden_vectors/
â”‚   â”œâ”€â”€ residual_*.hex               âœ…
â”‚   â”œâ”€â”€ batch_*.hex                  âœ…
â”‚   â””â”€â”€ ...
â”œâ”€â”€ tb/
â”‚   â”œâ”€â”€ tb_residual_block.v          âœ…
â”‚   â”œâ”€â”€ tb_batch_inference.v         ğŸ”„
â”‚   â”œâ”€â”€ tb_mlp_2layer.v              TODO
â”‚   â”œâ”€â”€ tb_tiled_gemm_16x16.v        TODO
â”‚   â”œâ”€â”€ tb_conv2d_multichannel.v     TODO
â”‚   â””â”€â”€ tb_attention.v               TODO
â””â”€â”€ run_tests.sh                     (updated)
```

---

## Next Steps

1. Fix batch processing RTL test
2. Implement 2-Layer MLP (Python + RTL)
3. Implement Large Tiled GEMM (Python + RTL)
4. Implement Multi-Channel Conv2D (Python + RTL)
5. Implement Attention Score (Python + RTL)
6. Full regression test suite
